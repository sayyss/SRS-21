{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "521c2633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from arc_vae.data_loader import Tasks,torchify, Grids, Grids_n\n",
    "from arc_vae.vae.models import vae\n",
    "from arc_vae.vae.models import losses\n",
    "import torch.nn.functional as F\n",
    "from arc_vae.vae import training\n",
    "from arc_vae.utils import arc_to_image, visualize_grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9a780e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, kernel_size=(4,4,2,1), stride=2):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        kernel_1,kernel_2,kernel_3,kernel_4 = kernel_size\n",
    "        self.strided_conv_1 = nn.Conv2d(input_dim, hidden_dim, kernel_1, stride, padding=1)\n",
    "        self.strided_conv_2 = nn.Conv2d(hidden_dim, hidden_dim, kernel_2, stride, padding=1)\n",
    "        \n",
    "        self.residual_conv_1 = nn.Conv2d(hidden_dim, hidden_dim, kernel_2, padding=1)\n",
    "        self.residual_conv_2 = nn.Conv2d(hidden_dim, output_dim, kernel_4, padding=0)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = self.strided_conv_1(x)\n",
    "        x = self.strided_conv_2(x)\n",
    "        \n",
    "        x = F.relu(x)\n",
    "        #print(x.shape)\n",
    "        y = self.residual_conv_1(x)\n",
    "        #print(y.shape)\n",
    "        y = y+x\n",
    "        \n",
    "        x = F.relu(y)\n",
    "        y = self.residual_conv_2(x)\n",
    "        y = y+x\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, kernel_sizes=(1, 3, 2, 2), stride=2):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        kernel_1, kernel_2, kernel_3, kernel_4 = kernel_sizes\n",
    "        \n",
    "        self.residual_conv_1 = nn.Conv2d(input_dim, hidden_dim, kernel_1, padding=0)\n",
    "        self.residual_conv_2 = nn.Conv2d(hidden_dim, hidden_dim, kernel_2, padding=1)\n",
    "        \n",
    "        self.strided_t_conv_1 = nn.ConvTranspose2d(hidden_dim, hidden_dim, kernel_2, stride, padding=0)\n",
    "        self.strided_t_conv_2 = nn.ConvTranspose2d(hidden_dim, output_dim, kernel_3, stride, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        y = self.residual_conv_1(x)\n",
    "        y = y+x\n",
    "        x = F.relu(y)\n",
    "        \n",
    "        y = self.residual_conv_2(x)\n",
    "        y = y+x\n",
    "        y = F.relu(y) # 2 2\n",
    "        \n",
    "        y = self.strided_t_conv_1(y) #4 4\n",
    "        \n",
    "        y = self.strided_t_conv_2(y) # 8 8\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n",
    "class VQEmbeddingEMA(nn.Module):\n",
    "    def __init__(self, n_embeddings, embedding_dim, commitment_cost=0.25, decay=0.999, epsilon=1e-5):\n",
    "        super(VQEmbeddingEMA, self).__init__()\n",
    "        self.commitment_cost = commitment_cost\n",
    "        self.decay = decay\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        init_bound = 1 / n_embeddings\n",
    "        embedding = torch.Tensor(n_embeddings, embedding_dim)\n",
    "        embedding.uniform_(-init_bound, init_bound)\n",
    "        self.register_buffer(\"embedding\", embedding)\n",
    "        self.register_buffer(\"ema_count\", torch.zeros(n_embeddings))\n",
    "        self.register_buffer(\"ema_weight\", self.embedding.clone())\n",
    "\n",
    "    def encode(self, x):\n",
    "        M, D = self.embedding.size()\n",
    "        x_flat = x.detach().reshape(-1, D)\n",
    "\n",
    "        distances = torch.addmm(torch.sum(self.embedding ** 2, dim=1) +\n",
    "                    torch.sum(x_flat ** 2, dim=1, keepdim=True),\n",
    "                                x_flat, self.embedding.t(),\n",
    "                                alpha=-2.0, beta=1.0)\n",
    "\n",
    "        indices = torch.argmin(distances.float(), dim=-1)\n",
    "        quantized = F.embedding(indices, self.embedding)\n",
    "        quantized = quantized.view_as(x)\n",
    "        return quantized, indices.view(x.size(0), x.size(1))\n",
    "    \n",
    "    def retrieve_random_codebook(self, random_indices):\n",
    "        quantized = F.embedding(random_indices, self.embedding)\n",
    "        quantized = quantized.transpose(1, 3)\n",
    "        \n",
    "        return quantized\n",
    "\n",
    "    def forward(self, x):\n",
    "        M, D = self.embedding.size()\n",
    "        x_flat = x.detach().reshape(-1, D)\n",
    "        \n",
    "        distances = torch.addmm(torch.sum(self.embedding ** 2, dim=1) +\n",
    "                                torch.sum(x_flat ** 2, dim=1, keepdim=True),\n",
    "                                x_flat, self.embedding.t(),\n",
    "                                alpha=-2.0, beta=1.0)\n",
    "\n",
    "        indices = torch.argmin(distances.float(), dim=-1)\n",
    "        encodings = F.one_hot(indices, M).float()\n",
    "        quantized = F.embedding(indices, self.embedding)\n",
    "        quantized = quantized.view_as(x)\n",
    "        \n",
    "        if self.training:\n",
    "            self.ema_count = self.decay * self.ema_count + (1 - self.decay) * torch.sum(encodings, dim=0)\n",
    "            n = torch.sum(self.ema_count)\n",
    "            self.ema_count = (self.ema_count + self.epsilon) / (n + M * self.epsilon) * n\n",
    "\n",
    "            dw = torch.matmul(encodings.t(), x_flat)\n",
    "            self.ema_weight = self.decay * self.ema_weight + (1 - self.decay) * dw\n",
    "            self.embedding = self.ema_weight / self.ema_count.unsqueeze(-1)\n",
    "\n",
    "        codebook_loss = F.mse_loss(x.detach(), quantized)\n",
    "        e_latent_loss = F.mse_loss(x, quantized.detach())\n",
    "        commitment_loss = self.commitment_cost * e_latent_loss\n",
    "\n",
    "        quantized = x + (quantized - x).detach()\n",
    "\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "\n",
    "        return quantized, commitment_loss, codebook_loss, perplexity\n",
    "\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, Encoder, Codebook, Decoder):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder\n",
    "        self.codebook = Codebook\n",
    "        self.decoder = Decoder\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z_quantized, commitment_loss, codebook_loss, perplexity = self.codebook(z)\n",
    "        x_hat = self.decoder(z_quantized)\n",
    "        \n",
    "        return x_hat, commitment_loss, codebook_loss, perplexity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c438297b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_dim, output_dim, hidden_dim, n_embeddings):\n",
    "    encoder = Encoder(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=hidden_dim)\n",
    "    codebook = VQEmbeddingEMA(n_embeddings=n_embeddings, embedding_dim=hidden_dim)\n",
    "    decoder = Decoder(input_dim=hidden_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "    \n",
    "    model = Model(Encoder=encoder, Codebook=codebook, Decoder=decoder).to('cuda')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5f7a2ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(batch_size, eval=False):\n",
    "    \n",
    "    tasks = Grids_n(eval=eval)\n",
    "    loader = DataLoader(tasks, batch_size=batch_size, drop_last=True)\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5aee3b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def weight_reset(m):\n",
    "    \n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear) or isinstance(m,nn.ConvTranspose2d):\n",
    "        m.reset_parameters()\n",
    "\n",
    "def get_optimizer(learning_rate, model):\n",
    "    \n",
    "    return optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "06789c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.MSELoss()\n",
    "\n",
    "def train(model, data_loader, optimizer, epochs):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        for data in data_loader:\n",
    "            data = data.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            out, commitment_loss, codebook_loss, perplexity = model(data)\n",
    "            \n",
    "            recon_loss = loss_func(out,data)\n",
    "            loss = recon_loss + commitment_loss + codebook_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'epoch: {epoch}, recon_loss: {recon_loss.item()}, perplexity: {perplexity.item()}, \\\n",
    "        commitment_loss: {commitment_loss.item()}, codebook_loss: {codebook_loss}, overall_loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "49ae588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    batch_size = 512\n",
    "    img_size = (10,10)\n",
    "    learning_rate = 2e-4\n",
    "    epochs = 50\n",
    "    \n",
    "    input_dim = 11\n",
    "    output_dim = 11\n",
    "    hidden_dim = 128\n",
    "    n_embeddings= 768\n",
    "    \n",
    "    train_loader = get_dataloader(batch_size=batch_size)\n",
    "    model = get_model(input_dim,output_dim, hidden_dim, n_embeddings)\n",
    "    optimizer = get_optimizer(learning_rate, model)\n",
    "    \n",
    "    model.apply(weight_reset)\n",
    "    train(model, train_loader, optimizer, epochs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e1d25f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, recon_loss: 0.02326076850295067, perplexity: 37.11821746826172,         commitment_loss: 0.0040822699666023254, codebook_loss: 0.016329079866409302, overall_loss: 0.043672118335962296\n",
      "epoch: 1, recon_loss: 0.02044995129108429, perplexity: 53.39557647705078,         commitment_loss: 0.004462371114641428, codebook_loss: 0.017849484458565712, overall_loss: 0.04276180639863014\n",
      "epoch: 2, recon_loss: 0.019653448835015297, perplexity: 59.32538604736328,         commitment_loss: 0.004909425973892212, codebook_loss: 0.019637703895568848, overall_loss: 0.04420057684183121\n",
      "epoch: 3, recon_loss: 0.019251354038715363, perplexity: 62.018890380859375,         commitment_loss: 0.005240867845714092, codebook_loss: 0.02096347138285637, overall_loss: 0.0454556941986084\n",
      "epoch: 4, recon_loss: 0.018968170508742332, perplexity: 64.5673828125,         commitment_loss: 0.005526754539459944, codebook_loss: 0.022107018157839775, overall_loss: 0.04660194367170334\n",
      "epoch: 5, recon_loss: 0.018813177943229675, perplexity: 65.61708068847656,         commitment_loss: 0.005774908699095249, codebook_loss: 0.023099634796380997, overall_loss: 0.04768772050738335\n",
      "epoch: 6, recon_loss: 0.018622945994138718, perplexity: 66.38194274902344,         commitment_loss: 0.005982526578009129, codebook_loss: 0.023930106312036514, overall_loss: 0.048535577952861786\n",
      "epoch: 7, recon_loss: 0.01848321035504341, perplexity: 67.17516326904297,         commitment_loss: 0.006155319511890411, codebook_loss: 0.024621278047561646, overall_loss: 0.04925980791449547\n",
      "epoch: 8, recon_loss: 0.018366504460573196, perplexity: 67.92012023925781,         commitment_loss: 0.006299232132732868, codebook_loss: 0.025196928530931473, overall_loss: 0.04986266419291496\n",
      "epoch: 9, recon_loss: 0.018270928412675858, perplexity: 69.46385955810547,         commitment_loss: 0.00643598148599267, codebook_loss: 0.02574392594397068, overall_loss: 0.05045083537697792\n",
      "epoch: 10, recon_loss: 0.018180428072810173, perplexity: 69.55248260498047,         commitment_loss: 0.00654634740203619, codebook_loss: 0.02618538960814476, overall_loss: 0.05091216415166855\n",
      "epoch: 11, recon_loss: 0.01808180660009384, perplexity: 71.24919128417969,         commitment_loss: 0.006647950038313866, codebook_loss: 0.026591800153255463, overall_loss: 0.05132155865430832\n",
      "epoch: 12, recon_loss: 0.018018968403339386, perplexity: 72.45741271972656,         commitment_loss: 0.0067571671679615974, codebook_loss: 0.02702866867184639, overall_loss: 0.0518048033118248\n",
      "epoch: 13, recon_loss: 0.01797463186085224, perplexity: 73.43517303466797,         commitment_loss: 0.006862933281809092, codebook_loss: 0.027451733127236366, overall_loss: 0.05228929966688156\n",
      "epoch: 14, recon_loss: 0.017937643453478813, perplexity: 73.9412841796875,         commitment_loss: 0.006983337923884392, codebook_loss: 0.027933351695537567, overall_loss: 0.05285433307290077\n",
      "epoch: 15, recon_loss: 0.01789039745926857, perplexity: 75.02600860595703,         commitment_loss: 0.007079416885972023, codebook_loss: 0.028317667543888092, overall_loss: 0.053287483751773834\n",
      "epoch: 16, recon_loss: 0.017854787409305573, perplexity: 75.33840942382812,         commitment_loss: 0.00715619046241045, codebook_loss: 0.0286247618496418, overall_loss: 0.05363573879003525\n",
      "epoch: 17, recon_loss: 0.01780109480023384, perplexity: 75.99256896972656,         commitment_loss: 0.007217440754175186, codebook_loss: 0.028869763016700745, overall_loss: 0.05388829857110977\n",
      "epoch: 18, recon_loss: 0.017754504457116127, perplexity: 77.32388305664062,         commitment_loss: 0.007280151825398207, codebook_loss: 0.029120607301592827, overall_loss: 0.05415526404976845\n",
      "epoch: 19, recon_loss: 0.01769392378628254, perplexity: 79.02168273925781,         commitment_loss: 0.007349345367401838, codebook_loss: 0.029397381469607353, overall_loss: 0.05444065108895302\n",
      "epoch: 20, recon_loss: 0.01762697659432888, perplexity: 79.54786682128906,         commitment_loss: 0.007411083206534386, codebook_loss: 0.029644332826137543, overall_loss: 0.05468239262700081\n",
      "epoch: 21, recon_loss: 0.017603322863578796, perplexity: 80.06831359863281,         commitment_loss: 0.0074746692553162575, codebook_loss: 0.02989867702126503, overall_loss: 0.05497666820883751\n",
      "epoch: 22, recon_loss: 0.017561864107847214, perplexity: 80.9349136352539,         commitment_loss: 0.00755100604146719, codebook_loss: 0.03020402416586876, overall_loss: 0.05531689524650574\n",
      "epoch: 23, recon_loss: 0.017537839710712433, perplexity: 81.8974609375,         commitment_loss: 0.007607164792716503, codebook_loss: 0.030428659170866013, overall_loss: 0.05557366460561752\n",
      "epoch: 24, recon_loss: 0.017502591013908386, perplexity: 82.43421936035156,         commitment_loss: 0.007667195983231068, codebook_loss: 0.03066878393292427, overall_loss: 0.05583856999874115\n",
      "epoch: 25, recon_loss: 0.017476234585046768, perplexity: 83.22572326660156,         commitment_loss: 0.007705732248723507, codebook_loss: 0.030822928994894028, overall_loss: 0.05600489675998688\n",
      "epoch: 26, recon_loss: 0.017428874969482422, perplexity: 83.24056243896484,         commitment_loss: 0.0077498909085989, codebook_loss: 0.0309995636343956, overall_loss: 0.05617833137512207\n",
      "epoch: 27, recon_loss: 0.01741808094084263, perplexity: 83.9609375,         commitment_loss: 0.007763860281556845, codebook_loss: 0.03105544112622738, overall_loss: 0.05623738467693329\n",
      "epoch: 28, recon_loss: 0.017402054741978645, perplexity: 84.43385314941406,         commitment_loss: 0.00780874490737915, codebook_loss: 0.0312349796295166, overall_loss: 0.05644577741622925\n",
      "epoch: 29, recon_loss: 0.017370132729411125, perplexity: 84.92269897460938,         commitment_loss: 0.007838167250156403, codebook_loss: 0.03135266900062561, overall_loss: 0.05656097084283829\n",
      "epoch: 30, recon_loss: 0.017309892922639847, perplexity: 85.43858337402344,         commitment_loss: 0.007876401767134666, codebook_loss: 0.031505607068538666, overall_loss: 0.05669189989566803\n",
      "epoch: 31, recon_loss: 0.01730545423924923, perplexity: 86.11328887939453,         commitment_loss: 0.007901737466454506, codebook_loss: 0.031606949865818024, overall_loss: 0.05681414157152176\n",
      "epoch: 32, recon_loss: 0.017299044877290726, perplexity: 86.2469253540039,         commitment_loss: 0.007910600863397121, codebook_loss: 0.031642403453588486, overall_loss: 0.05685205012559891\n",
      "epoch: 33, recon_loss: 0.017262445762753487, perplexity: 87.07079315185547,         commitment_loss: 0.007937807589769363, codebook_loss: 0.031751230359077454, overall_loss: 0.05695148557424545\n",
      "epoch: 34, recon_loss: 0.017244987189769745, perplexity: 87.13379669189453,         commitment_loss: 0.007957341149449348, codebook_loss: 0.031829364597797394, overall_loss: 0.05703169107437134\n",
      "epoch: 35, recon_loss: 0.017226697877049446, perplexity: 87.2479248046875,         commitment_loss: 0.007968234829604626, codebook_loss: 0.0318729393184185, overall_loss: 0.05706787109375\n",
      "epoch: 36, recon_loss: 0.017211418598890305, perplexity: 87.96329498291016,         commitment_loss: 0.007978294976055622, codebook_loss: 0.03191317990422249, overall_loss: 0.05710289254784584\n",
      "epoch: 37, recon_loss: 0.01720443367958069, perplexity: 88.67196655273438,         commitment_loss: 0.007994613610208035, codebook_loss: 0.03197845444083214, overall_loss: 0.057177502661943436\n",
      "epoch: 38, recon_loss: 0.017188694328069687, perplexity: 89.35269165039062,         commitment_loss: 0.008002732880413532, codebook_loss: 0.03201093152165413, overall_loss: 0.057202357798814774\n",
      "epoch: 39, recon_loss: 0.017167551442980766, perplexity: 89.16112518310547,         commitment_loss: 0.008041925728321075, codebook_loss: 0.0321677029132843, overall_loss: 0.05737718194723129\n",
      "epoch: 40, recon_loss: 0.017144091427326202, perplexity: 90.23532104492188,         commitment_loss: 0.008040539920330048, codebook_loss: 0.03216215968132019, overall_loss: 0.05734679102897644\n",
      "epoch: 41, recon_loss: 0.017122579738497734, perplexity: 90.45377349853516,         commitment_loss: 0.008053415454924107, codebook_loss: 0.032213661819696426, overall_loss: 0.05738965794444084\n",
      "epoch: 42, recon_loss: 0.017117077484726906, perplexity: 90.68801879882812,         commitment_loss: 0.00808210950344801, codebook_loss: 0.03232843801379204, overall_loss: 0.05752762407064438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 43, recon_loss: 0.017083361744880676, perplexity: 90.90380859375,         commitment_loss: 0.008104391396045685, codebook_loss: 0.03241756558418274, overall_loss: 0.0576053187251091\n",
      "epoch: 44, recon_loss: 0.017076939344406128, perplexity: 91.97118377685547,         commitment_loss: 0.008121263235807419, codebook_loss: 0.032485052943229675, overall_loss: 0.05768325552344322\n",
      "epoch: 45, recon_loss: 0.017056988552212715, perplexity: 92.54637908935547,         commitment_loss: 0.008133601397275925, codebook_loss: 0.0325344055891037, overall_loss: 0.05772499740123749\n",
      "epoch: 46, recon_loss: 0.0170341394841671, perplexity: 92.45829772949219,         commitment_loss: 0.008131584152579308, codebook_loss: 0.03252633661031723, overall_loss: 0.05769205838441849\n",
      "epoch: 47, recon_loss: 0.017005223780870438, perplexity: 93.01338958740234,         commitment_loss: 0.008141509257256985, codebook_loss: 0.03256603702902794, overall_loss: 0.057712770998477936\n",
      "epoch: 48, recon_loss: 0.016985498368740082, perplexity: 92.95481872558594,         commitment_loss: 0.0081571564078331, codebook_loss: 0.0326286256313324, overall_loss: 0.05777128040790558\n",
      "epoch: 49, recon_loss: 0.01696430891752243, perplexity: 92.60553741455078,         commitment_loss: 0.008166870102286339, codebook_loss: 0.032667480409145355, overall_loss: 0.057798661291599274\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d42c6b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496f7af7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
